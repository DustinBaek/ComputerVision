{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  0\n",
      "GeForce GTX 1660 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\cuda\\memory.py:344: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 0 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "# Additional Infos\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(GPU_NUM)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### hyperparameters\n",
    "class HParams():\n",
    "    def __init__(self):\n",
    "        self.data_location = 'cat.npz'\n",
    "        self.enc_hidden_size = 256\n",
    "        self.dec_hidden_size = 512\n",
    "        self.Nz = 128\n",
    "        self.M = 20\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.4\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# load and prepare data\n",
    "def max_size(data):\n",
    "    \"\"\"larger sequence length in the data set\"\"\"\n",
    "    sizes = [len(seq) for seq in data]\n",
    "    return max(sizes)\n",
    "\n",
    "def purify(strokes):\n",
    "    \"\"\"removes to small or too long sequences + removes large gaps\"\"\"\n",
    "    data = []\n",
    "    for seq in strokes:\n",
    "        if seq.shape[0] <= hp.max_seq_length and seq.shape[0] > 10:\n",
    "            seq = np.minimum(seq, 1000)\n",
    "            seq = np.maximum(seq, -1000)\n",
    "            seq = np.array(seq, dtype=np.float32)\n",
    "            data.append(seq)\n",
    "    return data\n",
    "\n",
    "def calculate_normalizing_scale_factor(strokes):\n",
    "    \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
    "    data = []\n",
    "    for i in range(len(strokes)):\n",
    "        for j in range(len(strokes[i])):\n",
    "            data.append(strokes[i][j, 0])\n",
    "            data.append(strokes[i][j, 1])\n",
    "    data = np.array(data)\n",
    "    return np.std(data)\n",
    "\n",
    "def normalize(strokes):\n",
    "    \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
    "    data = []\n",
    "    scale_factor = calculate_normalizing_scale_factor(strokes)\n",
    "    for seq in strokes:\n",
    "        seq[:, 0:2] /= scale_factor\n",
    "        data.append(seq)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "dataset = np.load(hp.data_location, encoding='latin1')\n",
    "\n",
    "data = dataset['train']\n",
    "data = purify(data)\n",
    "data = normalize(data)\n",
    "Nmax = max_size(data)\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## function to generate a batch:\n",
    "def make_batch(batch_size):\n",
    "    batch_idx = np.random.choice(len(data),batch_size)\n",
    "    batch_sequences = [data[idx] for idx in batch_idx]\n",
    "    strokes = []\n",
    "    lengths = []\n",
    "    indice = 0\n",
    "    for seq in batch_sequences:\n",
    "        len_seq = len(seq[:,0])\n",
    "        new_seq = np.zeros((Nmax,5))\n",
    "        new_seq[:len_seq,:2] = seq[:,:2]\n",
    "        new_seq[:len_seq-1,2] = 1-seq[:-1,2]\n",
    "        new_seq[:len_seq,3] = seq[:,2]\n",
    "        new_seq[(len_seq-1):,4] = 1\n",
    "        new_seq[len_seq-1,2:4] = 0\n",
    "        lengths.append(len(seq[:,0]))\n",
    "        strokes.append(new_seq)\n",
    "        indice += 1\n",
    "\n",
    "    if use_cuda:\n",
    "        batch = Variable(torch.from_numpy(np.stack(strokes,1)).cuda().float())\n",
    "    else:\n",
    "        batch = Variable(torch.from_numpy(np.stack(strokes,1)).float())\n",
    "    return batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ adaptive lr\n",
    "def lr_decay(optimizer):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr']>hp.min_lr:\n",
    "            param_group['lr'] *= hp.lr_decay\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# encoder and decoder modules\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # bidirectional lstm:\n",
    "        self.lstm = nn.LSTM(5, hp.enc_hidden_size, \\\n",
    "            dropout=hp.dropout, bidirectional=True)\n",
    "        # create mu and sigma from lstm's last output:\n",
    "        self.fc_mu = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
    "        self.fc_sigma = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
    "        # active dropout:\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then must init with zeros\n",
    "            if use_cuda:\n",
    "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
    "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
    "            else:\n",
    "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
    "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
    "            hidden_cell = (hidden, cell)\n",
    "        _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "        hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "        # mu and sigma:\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat/2.)\n",
    "        # N ~ N(0,1)\n",
    "        z_size = mu.size()\n",
    "        if use_cuda:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "        else:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "        z = mu + sigma*N\n",
    "        # mu and sigma_hat are needed for LKL loss\n",
    "        return z, mu, sigma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # to init hidden and cell from z:\n",
    "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.dec_hidden_size)\n",
    "        # unidirectional lstm:\n",
    "        self.lstm = nn.LSTM(hp.Nz+5, hp.dec_hidden_size, dropout=hp.dropout)\n",
    "        # create proba distribution parameters from hiddens:\n",
    "        self.fc_params = nn.Linear(hp.dec_hidden_size,6*hp.M+3)\n",
    "\n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then we must init from z\n",
    "            hidden,cell = torch.split(F.tanh(self.fc_hc(z)),hp.dec_hidden_size,1)\n",
    "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
    "        outputs,(hidden,cell) = self.lstm(inputs, hidden_cell)\n",
    "        # in training we feed the lstm with the whole input in one shot\n",
    "        # and use all outputs contained in 'outputs', while in generate\n",
    "        # mode we just feed with the last generated sample:\n",
    "        if self.training:\n",
    "            y = self.fc_params(outputs.view(-1, hp.dec_hidden_size))\n",
    "        else:\n",
    "            y = self.fc_params(hidden.view(-1, hp.dec_hidden_size))\n",
    "        # separate pen and mixture params:\n",
    "        params = torch.split(y,6,1)\n",
    "        params_mixture = torch.stack(params[:-1]) # trajectory\n",
    "        params_pen = params[-1] # pen up/down\n",
    "        # identify mixture params:\n",
    "        pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy = torch.split(params_mixture,1,2)\n",
    "        # preprocess params::\n",
    "        if self.training:\n",
    "            len_out = Nmax+1\n",
    "        else:\n",
    "            len_out = 1\n",
    "                                   \n",
    "        pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        sigma_x = torch.exp(sigma_x.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        sigma_y = torch.exp(sigma_y.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        rho_xy = torch.tanh(rho_xy.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        mu_x = mu_x.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
    "        mu_y = mu_y.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
    "        q = F.softmax(params_pen).view(len_out,-1,3)\n",
    "        return pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy,q,hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        if use_cuda:\n",
    "            self.encoder = EncoderRNN().cuda()\n",
    "            self.decoder = DecoderRNN().cuda()\n",
    "        else:\n",
    "            self.encoder = EncoderRNN()\n",
    "            self.decoder = DecoderRNN()\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
    "        self.eta_step = hp.eta_min\n",
    "\n",
    "    def make_target(self, batch, lengths):\n",
    "        if use_cuda:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).unsqueeze(0)\n",
    "        batch = torch.cat([batch, eos], 0)\n",
    "        mask = torch.zeros(Nmax+1, batch.size()[1])\n",
    "        for indice,length in enumerate(lengths):\n",
    "            mask[:length,indice] = 1\n",
    "        if use_cuda:\n",
    "            mask = mask.cuda()\n",
    "        dx = torch.stack([batch.data[:,:,0]]*hp.M,2)\n",
    "        dy = torch.stack([batch.data[:,:,1]]*hp.M,2)\n",
    "        p1 = batch.data[:,:,2]\n",
    "        p2 = batch.data[:,:,3]\n",
    "        p3 = batch.data[:,:,4]\n",
    "        p = torch.stack([p1,p2,p3],2)\n",
    "        return mask,dx,dy,p\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        batch, lengths = make_batch(hp.batch_size)\n",
    "        # encode:\n",
    "        z, self.mu, self.sigma = self.encoder(batch, hp.batch_size)\n",
    "        # create start of sequence:\n",
    "        if use_cuda:\n",
    "            sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).unsqueeze(0)\n",
    "        # had sos at the begining of the batch:\n",
    "        batch_init = torch.cat([sos, batch],0)\n",
    "        # expend z to be ready to concatenate with inputs:\n",
    "        z_stack = torch.stack([z]*(Nmax+1))\n",
    "        # inputs is concatenation of z and batch_inputs\n",
    "        inputs = torch.cat([batch_init, z_stack],2)\n",
    "        # decode:\n",
    "        self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "            self.rho_xy, self.q, _, _ = self.decoder(inputs, z)\n",
    "        # prepare targets:\n",
    "        mask,dx,dy,p = self.make_target(batch, lengths)\n",
    "        # prepare optimizers:\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        # update eta for LKL:\n",
    "        self.eta_step = 1-(1-hp.eta_min)*hp.R\n",
    "        # compute losses:\n",
    "        LKL = self.kullback_leibler_loss()\n",
    "        LR = self.reconstruction_loss(mask,dx,dy,p,epoch)\n",
    "        loss = LR + LKL\n",
    "        # gradient step\n",
    "        loss.backward()\n",
    "        # gradient cliping\n",
    "        nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
    "        nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n",
    "        # optim step\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        # some print and save:\n",
    "        if epoch%1==0:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            print('epoch',epoch,'loss',loss.item(),'LR',LR.item(),'LKL',LKL.item())\n",
    "#             print('epoch',epoch,'loss',loss.data[0],'LR',LR.data[0],'LKL',LKL.data[0])\n",
    "            self.encoder_optimizer = lr_decay(self.encoder_optimizer)\n",
    "            self.decoder_optimizer = lr_decay(self.decoder_optimizer)\n",
    "        if epoch%100==0:\n",
    "            #self.save(epoch)\n",
    "#             self.conditional_generation(epoch)\n",
    "            self.reconstruct()\n",
    "    \n",
    "    def bivariate_normal_pdf(self, dx, dy):\n",
    "        z_x = ((dx-self.mu_x)/self.sigma_x)**2\n",
    "        z_y = ((dy-self.mu_y)/self.sigma_y)**2\n",
    "        z_xy = (dx-self.mu_x)*(dy-self.mu_y)/(self.sigma_x*self.sigma_y)\n",
    "        z = z_x + z_y -2*self.rho_xy*z_xy\n",
    "        exp = torch.exp(-z/(2*(1-self.rho_xy**2)))\n",
    "        norm = 2*np.pi*self.sigma_x*self.sigma_y*torch.sqrt(1-self.rho_xy**2)\n",
    "        return exp/norm\n",
    "\n",
    "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
    "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
    "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
    "            /float(Nmax*hp.batch_size)\n",
    "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
    "        return LS+LP\n",
    "\n",
    "    def kullback_leibler_loss(self):\n",
    "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
    "            /float(hp.Nz*hp.batch_size)\n",
    "        if use_cuda:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min]).cuda()).detach()\n",
    "        else:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min])).detach()\n",
    "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
    "\n",
    "    def save(self, epoch):\n",
    "        sel = np.random.rand()\n",
    "        torch.save(self.encoder.state_dict(), \\\n",
    "            'encoderRNN_sel_%3f_epoch_%d.pth' % (sel,epoch))\n",
    "        torch.save(self.decoder.state_dict(), \\\n",
    "            'decoderRNN_sel_%3f_epoch_%d.pth' % (sel,epoch))\n",
    "\n",
    "    def load(self, encoder_name, decoder_name):\n",
    "        saved_encoder = torch.load(encoder_name)\n",
    "        saved_decoder = torch.load(decoder_name)\n",
    "        self.encoder.load_state_dict(saved_encoder)\n",
    "        self.decoder.load_state_dict(saved_decoder)\n",
    "\n",
    "    def conditional_generation(self, epoch):\n",
    "        batch,lengths = make_batch(1)\n",
    "        # should remove dropouts:\n",
    "        self.encoder.train(False)\n",
    "        self.decoder.train(False)\n",
    "        # encode:\n",
    "        z, _, _ = self.encoder(batch, 1)\n",
    "        if use_cuda:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1).cuda())\n",
    "        else:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1))\n",
    "        s = sos\n",
    "        seq_x = []\n",
    "        seq_y = []\n",
    "        seq_z = []\n",
    "        hidden_cell = None\n",
    "        for i in range(Nmax):\n",
    "            input = torch.cat([s,z.unsqueeze(0)],2)\n",
    "            # decode:\n",
    "            self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "                self.rho_xy, self.q, hidden, cell = \\\n",
    "                    self.decoder(input, z, hidden_cell)\n",
    "            hidden_cell = (hidden, cell)\n",
    "            # sample from parameters:\n",
    "            s, dx, dy, pen_down, eos = self.sample_next_state()\n",
    "#             s, dx, dy, pen_down, eos = self.sample_next()\n",
    "\n",
    "            #------\n",
    "            seq_x.append(dx)\n",
    "            seq_y.append(dy)\n",
    "            seq_z.append(pen_down)\n",
    "            if eos:\n",
    "                print(i)\n",
    "                break\n",
    "        # visualize result:\n",
    "        x_sample = np.cumsum(seq_x, 0)\n",
    "        y_sample = np.cumsum(seq_y, 0)\n",
    "        z_sample = np.array(seq_z)\n",
    "        sequence = np.stack([x_sample,y_sample,z_sample]).T\n",
    "        make_image(sequence, epoch)\n",
    "\n",
    "    def sample_next(self, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q):\n",
    "        pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q =\\\n",
    "            pi[0, 0, :], mu_x[0, 0, :], mu_y[0, 0, :], sigma_x[0,\n",
    "                                                               0, :], sigma_y[0, 0, :], rho_xy[0, 0, :], q[0, 0, :]\n",
    "        mu_x, mu_y, sigma_x, sigma_y, rho_xy =\\\n",
    "            mu_x.cpu().numpy(), mu_y.cpu().numpy(), sigma_x.cpu(\n",
    "            ).numpy(), sigma_y.cpu().numpy(), rho_xy.cpu().numpy()\n",
    "        M = pi.shape[0]\n",
    "        # offset\n",
    "        idx = np.random.choice(M, p=pi.cpu().numpy())\n",
    "        mean = [mu_x[idx], mu_y[idx]]\n",
    "        cov = [[sigma_x[idx] * sigma_x[idx], rho_xy[idx] * sigma_x[idx]*sigma_y[idx]],\n",
    "               [rho_xy[idx] * sigma_x[idx]*sigma_y[idx], sigma_y[idx] * sigma_y[idx]]]\n",
    "        xy = np.random.multivariate_normal(mean, cov, 1)\n",
    "        xy = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "        # pen\n",
    "        p = torch.tensor([0, 0, 0], device=device, dtype=torch.float)\n",
    "        idx = np.random.choice(3, p=q.cpu().numpy())\n",
    "        p[idx] = 1.0\n",
    "        p = p.unsqueeze(0)\n",
    "\n",
    "        return torch.cat([xy, p], dim=1).unsqueeze(0)\n",
    "    \n",
    "    def sample_next_state(self):\n",
    "\n",
    "        def adjust_temp(pi_pdf):\n",
    "            pi_pdf = np.log(pi_pdf)/hp.temperature\n",
    "            pi_pdf -= pi_pdf.max()\n",
    "            pi_pdf = np.exp(pi_pdf)\n",
    "            pi_pdf /= pi_pdf.sum()\n",
    "            return pi_pdf\n",
    "\n",
    "        # get mixture indice:\n",
    "        pi = self.pi.data[0,0,:].cpu().numpy()\n",
    "        pi = adjust_temp(pi)\n",
    "        pi_idx = np.random.choice(hp.M, p=pi)\n",
    "        # get pen state:\n",
    "        q = self.q.data[0,0,:].cpu().numpy()\n",
    "        q = adjust_temp(q)\n",
    "        q_idx = np.random.choice(3, p=q)\n",
    "        # get mixture params:\n",
    "        mu_x = self.mu_x.data[0,0,pi_idx]\n",
    "        mu_y = self.mu_y.data[0,0,pi_idx]\n",
    "        sigma_x = self.sigma_x.data[0,0,pi_idx]\n",
    "        sigma_y = self.sigma_y.data[0,0,pi_idx]\n",
    "        rho_xy = self.rho_xy.data[0,0,pi_idx]\n",
    "        \n",
    "        mean = [mu_x, mu_y]\n",
    "        sigma_x = sigma_x * np.sqrt(hp.temperature)\n",
    "        sigma_y = sigma_y * np.sqrt(hp.temperature)\n",
    "        cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],\\\n",
    "            [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "        import pdb; pdb.set_trace\n",
    "        xy = np.random.multivariate_normal(mean, cov, 1)\n",
    "        x=xy[0][0] \n",
    "        y=xy[0][1] \n",
    "#         x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
    "        next_state = torch.zeros(5)\n",
    "        next_state[0] = x\n",
    "        next_state[1] = y\n",
    "        next_state[q_idx+2] = 1\n",
    "        if use_cuda:\n",
    "            return Variable(next_state.cuda()).view(1,1,-1),x,y,q_idx==1,q_idx==2\n",
    "        else:\n",
    "            return Variable(next_state).view(1,1,-1),x,y,q_idx==1,q_idx==2\n",
    "#         return 1,2,3,4,5\n",
    "\n",
    "# def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "#     # inputs must be floats\n",
    "#     if greedy:\n",
    "#         return mu_x,mu_y\n",
    "#     mean = [mu_x, mu_y]\n",
    "#     sigma_x = sigma_x * np.sqrt(hp.temperature)\n",
    "#     sigma_y = sigma_y * np.sqrt(hp.temperature)\n",
    "#     cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],\\\n",
    "#         [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "#     import pdb; pdb.set_trace\n",
    "#     x = np.random.multivariate_normal(mean, cov, 1)\n",
    "#     x = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "#     return x[0][0], x[0][1]\n",
    "    def reconstruct(self, S):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            Nmax = S.shape[0]\n",
    "            batch_size = S.shape[1]\n",
    "            s_i = torch.stack(\n",
    "                [torch.tensor([0, 0, 1, 0, 0], device=device, dtype=torch.float)] * batch_size, dim=0).unsqueeze(0)\n",
    "            output = s_i  # dummy\n",
    "            z, _, _ = self.encoder(S)\n",
    "            h0, c0 = torch.split(torch.tanh(self.decoder.fc_hc(z)),\n",
    "                                 self.decoder.dec_hidden_size, 1)\n",
    "            hidden_cell = (h0.unsqueeze(0).contiguous(),\n",
    "                           c0.unsqueeze(0).contiguous())\n",
    "            for i in range(Nmax):\n",
    "                (pi, mu_x, mu_y, sigma_x, sigma_y,\n",
    "                 rho_xy, q), hidden_cell = self.decoder(s_i, z, hidden_cell)\n",
    "                s_i = self.sample_next(\n",
    "                    pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q)\n",
    "                output = torch.cat([output, s_i], dim=0)\n",
    "                if output[-1, 0, 4] == 1:\n",
    "                    break\n",
    "\n",
    "            output = output[1:, :, :]  # remove dummy\n",
    "            return output\n",
    "\n",
    "    def sample_next(self, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q):\n",
    "        pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q =\\\n",
    "            pi[0, 0, :], mu_x[0, 0, :], mu_y[0, 0, :], sigma_x[0,\n",
    "                                                               0, :], sigma_y[0, 0, :], rho_xy[0, 0, :], q[0, 0, :]\n",
    "        mu_x, mu_y, sigma_x, sigma_y, rho_xy =\\\n",
    "            mu_x.cpu().numpy(), mu_y.cpu().numpy(), sigma_x.cpu(\n",
    "            ).numpy(), sigma_y.cpu().numpy(), rho_xy.cpu().numpy()\n",
    "        M = pi.shape[0]\n",
    "        # offset\n",
    "        idx = np.random.choice(M, p=pi.cpu().numpy())\n",
    "        mean = [mu_x[idx], mu_y[idx]]\n",
    "        cov = [[sigma_x[idx] * sigma_x[idx], rho_xy[idx] * sigma_x[idx]*sigma_y[idx]],\n",
    "               [rho_xy[idx] * sigma_x[idx]*sigma_y[idx], sigma_y[idx] * sigma_y[idx]]]\n",
    "        xy = np.random.multivariate_normal(mean, cov, 1)\n",
    "        xy = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "        # pen\n",
    "        p = torch.tensor([0, 0, 0], device=device, dtype=torch.float)\n",
    "        idx = np.random.choice(3, p=q.cpu().numpy())\n",
    "        p[idx] = 1.0\n",
    "        p = p.unsqueeze(0)\n",
    "\n",
    "        return torch.cat([xy, p], dim=1).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image(sequence, epoch, name='_output_'):\n",
    "    \"\"\"plot drawing with separated strokes\"\"\"\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for s in strokes:\n",
    "        plt.plot(s[:,0],-s[:,1])\n",
    "    canvas = plt.get_current_fig_manager().canvas\n",
    "    canvas.draw()\n",
    "    pil_image = PIL.Image.frombytes('RGB', canvas.get_width_height(),\n",
    "                 canvas.tostring_rgb())\n",
    "    name = str(epoch)+name+'.jpg'\n",
    "    pil_image.save(name,\"JPEG\")\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.44055982]]), array([[-1.59296364]]), array([[0.5278084]]), array([[0.87765278]]), array([[0.16018935]]), array([[-0.11968494]]), array([[0.64920988]]), array([[0.68651249]]), array([[0.08941199]]), array([[-2.07039278]])]\n"
     ]
    }
   ],
   "source": [
    "arrays = [np.random.randn(1, 1) for _ in range(10)]\n",
    "print(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08773955 -0.75696939  0.00500692 -0.41489343]\n",
      " [-2.05989091  0.03713604 -0.8290457   0.50611965]\n",
      " [ 0.22814642  0.4164204  -1.5747917  -0.11221588]]\n",
      "[[-0.0441888   0.55901155 -0.48779465 -0.15579444]\n",
      " [-1.24987784  0.80118306  0.30387306 -0.28900327]\n",
      " [ 0.20882738  0.11877047 -0.76264632  0.53039523]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.08773955, -0.75696939,  0.00500692, -0.41489343],\n",
       "        [-2.05989091,  0.03713604, -0.8290457 ,  0.50611965],\n",
       "        [ 0.22814642,  0.4164204 , -1.5747917 , -0.11221588]],\n",
       "\n",
       "       [[-0.0441888 ,  0.55901155, -0.48779465, -0.15579444],\n",
       "        [-1.24987784,  0.80118306,  0.30387306, -0.28900327],\n",
       "        [ 0.20882738,  0.11877047, -0.76264632,  0.53039523]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.random.randn(3,4)\n",
    "print(x)\n",
    "y=np.random.randn(3,4)\n",
    "print(y)\n",
    "np.stack([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23544071  2.51186793]]\n",
      "torch.Size([1, 2])\n",
      "[2 0 0 0 1]\n",
      "tensor([1., 1., 1.], device='cuda:0')\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "mean = [1, 2]\n",
    "cov = [[1, 0], [0, 1]]  \n",
    "x = np.random.multivariate_normal(mean, cov, 1)\n",
    "print(x)\n",
    "x = torch.from_numpy(x).float().to(device)\n",
    "print(x.shape)\n",
    "\n",
    "p = torch.tensor([0, 0, 0], device=device, dtype=torch.float)\n",
    "idx = np.random.choice(3, 5)\n",
    "print(idx)\n",
    "p[idx] = 1.0\n",
    "print(p)\n",
    "p = p.unsqueeze(0)\n",
    "print(p.shape)\n",
    "z=torch.cat([x, p], dim=1).unsqueeze(0)\n",
    "print(z.shape)\n",
    "# print(x[0])\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = np.zeros(1)\n",
    "c = np.zeros(1)\n",
    "c = c/2**63\n",
    "\n",
    "b += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model=Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-9-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-15-63ddef2dc04f>:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-15-63ddef2dc04f>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 2.405754327774048 LR 2.4047493934631348 LKL 0.0010049500269815326\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-9-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-9-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-15-63ddef2dc04f>:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-15-63ddef2dc04f>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 2.4273736476898193 LR 2.4263687133789062 LKL 0.0010049500269815326\n",
      "epoch 2 loss 2.344931125640869 LR 2.343926191329956 LKL 0.0010049500269815326\n",
      "epoch 3 loss 2.3695778846740723 LR 2.368572950363159 LKL 0.0010049500269815326\n",
      "epoch 4 loss 2.3616034984588623 LR 2.360598564147949 LKL 0.0010049500269815326\n",
      "epoch 5 loss 2.240119457244873 LR 2.23911452293396 LKL 0.0010049500269815326\n",
      "epoch 6 loss 2.035344123840332 LR 2.034339189529419 LKL 0.0010049500269815326\n",
      "epoch 7 loss 2.218203544616699 LR 2.217198610305786 LKL 0.0010049500269815326\n",
      "epoch 8 loss 2.0895864963531494 LR 2.0885815620422363 LKL 0.0010049500269815326\n",
      "epoch 9 loss 1.9057108163833618 LR 1.9047058820724487 LKL 0.0010049500269815326\n",
      "epoch 10 loss 1.95341157913208 LR 1.952406644821167 LKL 0.0010049500269815326\n",
      "epoch 11 loss 1.925418496131897 LR 1.9244135618209839 LKL 0.0010049500269815326\n",
      "epoch 12 loss 1.7686957120895386 LR 1.7676907777786255 LKL 0.0010049500269815326\n",
      "epoch 13 loss 1.688295841217041 LR 1.687290906906128 LKL 0.0010049500269815326\n",
      "epoch 14 loss 1.7307970523834229 LR 1.7297921180725098 LKL 0.0010049500269815326\n",
      "epoch 15 loss 1.7507940530776978 LR 1.7497891187667847 LKL 0.0010049500269815326\n",
      "epoch 16 loss 1.6705164909362793 LR 1.6695115566253662 LKL 0.0010049500269815326\n",
      "epoch 17 loss 1.624547004699707 LR 1.623542070388794 LKL 0.0010049500269815326\n",
      "epoch 18 loss 1.586465835571289 LR 1.585460901260376 LKL 0.0010049500269815326\n",
      "epoch 19 loss 1.481813907623291 LR 1.480808973312378 LKL 0.0010049500269815326\n",
      "epoch 20 loss 1.5295006036758423 LR 1.5284956693649292 LKL 0.0010049500269815326\n",
      "epoch 21 loss 1.6529486179351807 LR 1.6519436836242676 LKL 0.0010049500269815326\n",
      "epoch 22 loss 1.5758897066116333 LR 1.5748847723007202 LKL 0.0010049500269815326\n",
      "epoch 23 loss 1.6080420017242432 LR 1.60703706741333 LKL 0.0010049500269815326\n",
      "epoch 24 loss 1.5296481847763062 LR 1.528643250465393 LKL 0.0010049500269815326\n",
      "epoch 25 loss 1.4060983657836914 LR 1.4050934314727783 LKL 0.0010049500269815326\n",
      "epoch 26 loss 1.4804134368896484 LR 1.4794085025787354 LKL 0.0010049500269815326\n",
      "epoch 27 loss 1.4575048685073853 LR 1.4564999341964722 LKL 0.0010049500269815326\n",
      "epoch 28 loss 1.5420935153961182 LR 1.541088581085205 LKL 0.0010049500269815326\n",
      "epoch 29 loss 1.5478579998016357 LR 1.5468530654907227 LKL 0.0010049500269815326\n",
      "epoch 30 loss 1.4242448806762695 LR 1.4232399463653564 LKL 0.0010049500269815326\n",
      "epoch 31 loss 1.4287315607070923 LR 1.4277266263961792 LKL 0.0010049500269815326\n",
      "epoch 32 loss 1.4383063316345215 LR 1.4373013973236084 LKL 0.0010049500269815326\n",
      "epoch 33 loss 1.4219939708709717 LR 1.4209890365600586 LKL 0.0010049500269815326\n",
      "epoch 34 loss 1.4177334308624268 LR 1.4167284965515137 LKL 0.0010049500269815326\n",
      "epoch 35 loss 1.3284311294555664 LR 1.3274261951446533 LKL 0.0010049500269815326\n",
      "epoch 36 loss 1.3878958225250244 LR 1.3868908882141113 LKL 0.0010049500269815326\n",
      "epoch 37 loss 1.394151210784912 LR 1.393146276473999 LKL 0.0010049500269815326\n",
      "epoch 38 loss 1.3382151126861572 LR 1.3372101783752441 LKL 0.0010049500269815326\n",
      "epoch 39 loss 1.3718481063842773 LR 1.3708431720733643 LKL 0.0010049500269815326\n",
      "epoch 40 loss 1.4352409839630127 LR 1.4342360496520996 LKL 0.0010049500269815326\n",
      "epoch 41 loss 1.378202199935913 LR 1.377197265625 LKL 0.0010049500269815326\n",
      "epoch 42 loss 1.2338292598724365 LR 1.2328243255615234 LKL 0.0010049500269815326\n",
      "epoch 43 loss 1.4163379669189453 LR 1.4153330326080322 LKL 0.0010049500269815326\n",
      "epoch 44 loss 1.365116000175476 LR 1.3640732765197754 LKL 0.0010426671942695975\n",
      "epoch 45 loss 1.3198796510696411 LR 1.318702220916748 LKL 0.0011774023296311498\n",
      "epoch 46 loss 1.3588736057281494 LR 1.3574934005737305 LKL 0.0013801602181047201\n",
      "epoch 47 loss 1.3799937963485718 LR 1.3783702850341797 LKL 0.0016234719660133123\n",
      "epoch 48 loss 1.2840018272399902 LR 1.2821444272994995 LKL 0.0018573669949546456\n",
      "epoch 49 loss 1.3330063819885254 LR 1.3308793306350708 LKL 0.0021270064171403646\n",
      "epoch 50 loss 1.2751108407974243 LR 1.272797703742981 LKL 0.002313188510015607\n",
      "epoch 51 loss 1.3110703229904175 LR 1.3085105419158936 LKL 0.002559805056080222\n",
      "epoch 52 loss 1.2461955547332764 LR 1.2432159185409546 LKL 0.0029796073213219643\n",
      "epoch 53 loss 1.3073642253875732 LR 1.3043012619018555 LKL 0.0030630158726125956\n",
      "epoch 54 loss 1.241797685623169 LR 1.238569974899292 LKL 0.003227744484320283\n",
      "epoch 55 loss 1.3343288898468018 LR 1.3309550285339355 LKL 0.003373870626091957\n",
      "epoch 56 loss 1.185681700706482 LR 1.1834697723388672 LKL 0.0022118762135505676\n",
      "epoch 57 loss 1.2730228900909424 LR 1.2714917659759521 LKL 0.0015310910530388355\n",
      "epoch 58 loss 1.1904606819152832 LR 1.1892421245574951 LKL 0.0012185540981590748\n",
      "epoch 59 loss 1.2090938091278076 LR 1.2080321311950684 LKL 0.0010616244981065392\n",
      "epoch 60 loss 1.16793954372406 LR 1.166934609413147 LKL 0.0010049500269815326\n",
      "epoch 61 loss 1.2142362594604492 LR 1.2132313251495361 LKL 0.0010049500269815326\n",
      "epoch 62 loss 1.1998369693756104 LR 1.1988320350646973 LKL 0.0010049500269815326\n",
      "epoch 63 loss 1.2086765766143799 LR 1.2076716423034668 LKL 0.0010049500269815326\n",
      "epoch 64 loss 1.3030378818511963 LR 1.3020329475402832 LKL 0.0010049500269815326\n",
      "epoch 65 loss 1.2736557722091675 LR 1.2726508378982544 LKL 0.0010049500269815326\n",
      "epoch 66 loss 1.2261531352996826 LR 1.2251482009887695 LKL 0.0010049500269815326\n",
      "epoch 67 loss 1.2326310873031616 LR 1.2316261529922485 LKL 0.0010049500269815326\n",
      "epoch 68 loss 1.2680352926254272 LR 1.2670303583145142 LKL 0.0010049500269815326\n",
      "epoch 69 loss 1.1748254299163818 LR 1.1738204956054688 LKL 0.0010049500269815326\n",
      "epoch 70 loss 1.177061676979065 LR 1.1760567426681519 LKL 0.0010049500269815326\n",
      "epoch 71 loss 1.24337899684906 LR 1.2423527240753174 LKL 0.001026279293000698\n",
      "epoch 72 loss 1.2038366794586182 LR 1.202739953994751 LKL 0.001096727792173624\n",
      "epoch 73 loss 1.150432825088501 LR 1.1492537260055542 LKL 0.0011791002470999956\n",
      "epoch 74 loss 1.2229822874069214 LR 1.2217121124267578 LKL 0.0012701896484941244\n",
      "epoch 75 loss 1.163727879524231 LR 1.162367343902588 LKL 0.0013605147833004594\n",
      "epoch 76 loss 1.166958212852478 LR 1.1654952764511108 LKL 0.001462886924855411\n",
      "epoch 77 loss 1.1936089992523193 LR 1.1920289993286133 LKL 0.0015799455577507615\n",
      "epoch 78 loss 1.0820634365081787 LR 1.0804119110107422 LKL 0.001651465892791748\n",
      "epoch 79 loss 1.1858853101730347 LR 1.184086799621582 LKL 0.0017984510632231832\n",
      "epoch 80 loss 1.1653255224227905 LR 1.1633810997009277 LKL 0.0019444201607257128\n",
      "epoch 81 loss 1.1295900344848633 LR 1.1274598836898804 LKL 0.002130128676071763\n",
      "epoch 82 loss 1.0971604585647583 LR 1.0948830842971802 LKL 0.002277364255860448\n",
      "epoch 83 loss 1.0497804880142212 LR 1.0473295450210571 LKL 0.0024508966598659754\n",
      "epoch 84 loss 1.0491183996200562 LR 1.0465537309646606 LKL 0.0025647201109677553\n",
      "epoch 85 loss 1.0922225713729858 LR 1.089521884918213 LKL 0.002700628014281392\n",
      "epoch 86 loss 1.0937191247940063 LR 1.0907673835754395 LKL 0.002951756352558732\n",
      "epoch 87 loss 1.116550087928772 LR 1.1133028268814087 LKL 0.0032472647726535797\n",
      "epoch 88 loss 1.0880953073501587 LR 1.08467435836792 LKL 0.0034209846053272486\n",
      "epoch 89 loss 1.0992783308029175 LR 1.0956248044967651 LKL 0.0036534867249429226\n",
      "epoch 90 loss 1.1205705404281616 LR 1.1168956756591797 LKL 0.003674883395433426\n",
      "epoch 91 loss 1.136671543121338 LR 1.13287353515625 LKL 0.003797957906499505\n",
      "epoch 92 loss 1.0654963254928589 LR 1.0619263648986816 LKL 0.0035699952859431505\n",
      "epoch 93 loss 1.2080682516098022 LR 1.204563021659851 LKL 0.003505233209580183\n",
      "epoch 94 loss 1.1192749738693237 LR 1.1158888339996338 LKL 0.0033861945848912\n",
      "epoch 95 loss 1.0309821367263794 LR 1.0277286767959595 LKL 0.0032534978818148375\n",
      "epoch 96 loss 1.1170697212219238 LR 1.113840103149414 LKL 0.003229606430977583\n",
      "epoch 97 loss 1.09873628616333 LR 1.0955923795700073 LKL 0.0031438625883311033\n",
      "epoch 98 loss 1.0700072050094604 LR 1.0671007633209229 LKL 0.002906465670093894\n",
      "epoch 99 loss 1.0320881605148315 LR 1.0292922258377075 LKL 0.0027959367725998163\n",
      "epoch 100 loss 0.9408408999443054 LR 0.9381026029586792 LKL 0.0027383037377148867\n",
      "0\n",
      "epoch 101 loss 0.9813591837882996 LR 0.9786332845687866 LKL 0.0027259215712547302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-9-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-9-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-15-63ddef2dc04f>:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-15-63ddef2dc04f>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102 loss 0.9752042293548584 LR 0.9722976684570312 LKL 0.002906557871028781\n",
      "epoch 103 loss 0.9629670977592468 LR 0.9599464535713196 LKL 0.0030206721276044846\n",
      "epoch 104 loss 0.9575383067131042 LR 0.9543203115463257 LKL 0.003217987483367324\n",
      "epoch 105 loss 1.0873547792434692 LR 1.0841126441955566 LKL 0.003242187201976776\n",
      "epoch 106 loss 1.0388665199279785 LR 1.0355372428894043 LKL 0.0033293161541223526\n",
      "epoch 107 loss 1.0555453300476074 LR 1.0521262884140015 LKL 0.0034190646838396788\n",
      "epoch 108 loss 1.0192492008209229 LR 1.015825867652893 LKL 0.0034233308397233486\n",
      "epoch 109 loss 1.081685185432434 LR 1.078113317489624 LKL 0.0035718984436243773\n",
      "epoch 110 loss 0.9586331248283386 LR 0.9548744559288025 LKL 0.0037586907856166363\n",
      "epoch 111 loss 0.9034527540206909 LR 0.8994507193565369 LKL 0.0040020085871219635\n",
      "epoch 112 loss 0.9377711415290833 LR 0.9335095882415771 LKL 0.004261581227183342\n",
      "epoch 113 loss 0.987449586391449 LR 0.9830882549285889 LKL 0.004361338447779417\n",
      "epoch 114 loss 0.9112076759338379 LR 0.9067967534065247 LKL 0.0044109090231359005\n",
      "epoch 115 loss 1.0024127960205078 LR 0.9978553652763367 LKL 0.004557448904961348\n",
      "epoch 116 loss 0.9672207236289978 LR 0.9626163840293884 LKL 0.004604347515851259\n",
      "epoch 117 loss 1.006412386894226 LR 1.001591682434082 LKL 0.00482068071141839\n",
      "epoch 118 loss 0.906110405921936 LR 0.9010568261146545 LKL 0.005053555127233267\n",
      "epoch 119 loss 0.9463340640068054 LR 0.9410181045532227 LKL 0.005315959453582764\n",
      "epoch 120 loss 0.9647444486618042 LR 0.9593786597251892 LKL 0.0053658136166632175\n",
      "epoch 121 loss 0.9347776770591736 LR 0.9294103384017944 LKL 0.005367321893572807\n",
      "epoch 122 loss 1.0035467147827148 LR 0.99827641248703 LKL 0.005270330235362053\n",
      "epoch 123 loss 0.9219785332679749 LR 0.9168558120727539 LKL 0.005122717469930649\n",
      "epoch 124 loss 0.8855927586555481 LR 0.8804970383644104 LKL 0.005095692351460457\n",
      "epoch 125 loss 0.8824647068977356 LR 0.8771241903305054 LKL 0.0053405254147946835\n",
      "epoch 126 loss 0.907912015914917 LR 0.9022790193557739 LKL 0.005632985383272171\n",
      "epoch 127 loss 0.9677006006240845 LR 0.961864173412323 LKL 0.005836445838212967\n",
      "epoch 128 loss 0.9222424626350403 LR 0.9163938164710999 LKL 0.0058486429043114185\n",
      "epoch 129 loss 0.9017530679702759 LR 0.8959444761276245 LKL 0.005808590445667505\n",
      "epoch 130 loss 0.9052751660346985 LR 0.8995630741119385 LKL 0.005712066777050495\n",
      "epoch 131 loss 0.8827177882194519 LR 0.8769016265869141 LKL 0.005816190503537655\n",
      "epoch 132 loss 0.8270679712295532 LR 0.8211703896522522 LKL 0.005897575058043003\n",
      "epoch 133 loss 0.8536710739135742 LR 0.8476703763008118 LKL 0.0060007162392139435\n",
      "epoch 134 loss 0.7955753803253174 LR 0.7895416617393494 LKL 0.00603370601311326\n",
      "epoch 135 loss 0.8483826518058777 LR 0.8419826626777649 LKL 0.006400007754564285\n",
      "epoch 136 loss 0.9217097163200378 LR 0.9149775505065918 LKL 0.006732183042913675\n",
      "epoch 137 loss 0.8700569272041321 LR 0.8634799122810364 LKL 0.00657702935859561\n",
      "epoch 138 loss 0.8759660720825195 LR 0.8694323301315308 LKL 0.006533759646117687\n",
      "epoch 139 loss 0.9200180768966675 LR 0.9134119749069214 LKL 0.006606114562600851\n",
      "epoch 140 loss 0.905051589012146 LR 0.8983073830604553 LKL 0.0067442236468195915\n",
      "epoch 141 loss 0.8777026534080505 LR 0.8706961870193481 LKL 0.0070064738392829895\n",
      "epoch 142 loss 0.9020865559577942 LR 0.8954792022705078 LKL 0.0066073499619960785\n",
      "epoch 143 loss 0.8952881693840027 LR 0.888898491859436 LKL 0.00638966541737318\n",
      "epoch 144 loss 0.7575278878211975 LR 0.7513688802719116 LKL 0.006159006152302027\n",
      "epoch 145 loss 0.8252079486846924 LR 0.8189511895179749 LKL 0.006256751716136932\n",
      "epoch 146 loss 0.8170421123504639 LR 0.81100994348526 LKL 0.006032145582139492\n",
      "epoch 147 loss 0.8568729162216187 LR 0.8507623672485352 LKL 0.006110569927841425\n",
      "epoch 148 loss 0.7982936501502991 LR 0.7919460535049438 LKL 0.006347609683871269\n",
      "epoch 149 loss 0.7878133058547974 LR 0.7814550995826721 LKL 0.006358216982334852\n",
      "epoch 150 loss 0.8475444912910461 LR 0.84112149477005 LKL 0.006422997917979956\n",
      "epoch 151 loss 0.8459889888763428 LR 0.839719295501709 LKL 0.006269713863730431\n",
      "epoch 152 loss 0.8023008108139038 LR 0.7959276437759399 LKL 0.006373156327754259\n",
      "epoch 153 loss 0.8290099501609802 LR 0.8227551579475403 LKL 0.006254767533391714\n",
      "epoch 154 loss 0.8206942081451416 LR 0.8145427703857422 LKL 0.00615142285823822\n",
      "epoch 155 loss 0.8117396831512451 LR 0.8051978349685669 LKL 0.006541842129081488\n",
      "epoch 156 loss 0.8382304310798645 LR 0.8314840197563171 LKL 0.006746404338628054\n",
      "epoch 157 loss 0.8204299211502075 LR 0.8135164976119995 LKL 0.006913415156304836\n",
      "epoch 158 loss 0.8425728678703308 LR 0.8357502818107605 LKL 0.006822606083005667\n",
      "epoch 159 loss 0.8199933171272278 LR 0.8130398392677307 LKL 0.006953448988497257\n",
      "epoch 160 loss 0.7490013241767883 LR 0.741750955581665 LKL 0.0072503904812037945\n",
      "epoch 161 loss 0.8078177571296692 LR 0.8003913164138794 LKL 0.007426442578434944\n",
      "epoch 162 loss 0.7818465232849121 LR 0.7744438648223877 LKL 0.007402676157653332\n",
      "epoch 163 loss 0.7850835919380188 LR 0.7776862978935242 LKL 0.00739730941131711\n",
      "epoch 164 loss 0.7540921568870544 LR 0.7464373707771301 LKL 0.007654789835214615\n",
      "epoch 165 loss 0.7165329456329346 LR 0.7090247869491577 LKL 0.007508148904889822\n",
      "epoch 166 loss 0.789654016494751 LR 0.7824517488479614 LKL 0.007202244829386473\n",
      "epoch 167 loss 0.7882741093635559 LR 0.7808853983879089 LKL 0.007388724014163017\n",
      "epoch 168 loss 0.7631368637084961 LR 0.7558448314666748 LKL 0.007292052730917931\n",
      "epoch 169 loss 0.8471017479896545 LR 0.8394328355789185 LKL 0.007668937090784311\n",
      "epoch 170 loss 0.7459011077880859 LR 0.7386190295219421 LKL 0.007282101083546877\n",
      "epoch 171 loss 0.7758502960205078 LR 0.7684959173202515 LKL 0.00735435588285327\n",
      "epoch 172 loss 0.7439162135124207 LR 0.7363018989562988 LKL 0.0076142894104123116\n",
      "epoch 173 loss 0.7747691869735718 LR 0.767211377620697 LKL 0.00755782937631011\n",
      "epoch 174 loss 0.76679927110672 LR 0.759703516960144 LKL 0.007095741108059883\n",
      "epoch 175 loss 0.822853684425354 LR 0.8161616921424866 LKL 0.006691962480545044\n",
      "epoch 176 loss 0.7499035596847534 LR 0.7430025339126587 LKL 0.006900998298078775\n",
      "epoch 177 loss 0.7788633704185486 LR 0.7718991041183472 LKL 0.0069642700254917145\n",
      "epoch 178 loss 0.7662237286567688 LR 0.759486734867096 LKL 0.006737008690834045\n",
      "epoch 179 loss 0.7643429636955261 LR 0.7581357955932617 LKL 0.0062071820721030235\n",
      "epoch 180 loss 0.735717236995697 LR 0.7295640110969543 LKL 0.006153251975774765\n",
      "epoch 181 loss 0.7415254712104797 LR 0.7355163097381592 LKL 0.006009159609675407\n",
      "epoch 182 loss 0.8246067762374878 LR 0.8185458183288574 LKL 0.006060929968953133\n",
      "epoch 183 loss 0.7511662244796753 LR 0.744777262210846 LKL 0.006388946436345577\n",
      "epoch 184 loss 0.7370471954345703 LR 0.7305731773376465 LKL 0.006474006921052933\n",
      "epoch 185 loss 0.7245844602584839 LR 0.7182060480117798 LKL 0.00637844018638134\n",
      "epoch 186 loss 0.7682591676712036 LR 0.7616809010505676 LKL 0.006578266620635986\n",
      "epoch 187 loss 0.7429112195968628 LR 0.7364873886108398 LKL 0.006423844490200281\n",
      "epoch 188 loss 0.7640186548233032 LR 0.7575687170028687 LKL 0.006449944339692593\n",
      "epoch 189 loss 0.7488590478897095 LR 0.7422013282775879 LKL 0.006657713558524847\n",
      "epoch 190 loss 0.7168323397636414 LR 0.7097110152244568 LKL 0.007121323607861996\n",
      "epoch 191 loss 0.7408542037010193 LR 0.7337085008621216 LKL 0.007145720534026623\n",
      "epoch 192 loss 0.7349757552146912 LR 0.7279394865036011 LKL 0.007036271039396524\n",
      "epoch 193 loss 0.7357732653617859 LR 0.7287005186080933 LKL 0.007072764448821545\n",
      "epoch 194 loss 0.6801931262016296 LR 0.6729577779769897 LKL 0.007235340308398008\n",
      "epoch 195 loss 0.7284924387931824 LR 0.7211731672286987 LKL 0.00731925293803215\n",
      "epoch 196 loss 0.7723161578178406 LR 0.7647939324378967 LKL 0.007522230036556721\n",
      "epoch 197 loss 0.6891584992408752 LR 0.6819123029708862 LKL 0.007246221881359816\n",
      "epoch 198 loss 0.6785626411437988 LR 0.6716831922531128 LKL 0.006879476830363274\n",
      "epoch 199 loss 0.7243537902832031 LR 0.7179794311523438 LKL 0.006374354474246502\n",
      "epoch 200 loss 0.7376132607460022 LR 0.7312432527542114 LKL 0.006369978655129671\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-9-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-9-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-15-63ddef2dc04f>:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-15-63ddef2dc04f>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 201 loss 0.7178159952163696 LR 0.7113367319107056 LKL 0.006479261908680201\n",
      "epoch 202 loss 0.747210681438446 LR 0.740930438041687 LKL 0.006280216854065657\n",
      "epoch 203 loss 0.7620486617088318 LR 0.7558327913284302 LKL 0.006215894594788551\n",
      "epoch 204 loss 0.7004297971725464 LR 0.6943261623382568 LKL 0.0061036208644509315\n",
      "epoch 205 loss 0.7479207515716553 LR 0.741606593132019 LKL 0.00631413608789444\n",
      "epoch 206 loss 0.7446556687355042 LR 0.7382557392120361 LKL 0.006399940699338913\n",
      "epoch 207 loss 0.6589229106903076 LR 0.6521995067596436 LKL 0.006723388563841581\n",
      "epoch 208 loss 0.7123401165008545 LR 0.7054097056388855 LKL 0.0069303959608078\n",
      "epoch 209 loss 0.7491775751113892 LR 0.7422913908958435 LKL 0.006886172108352184\n",
      "epoch 210 loss 0.7133777737617493 LR 0.7062698006629944 LKL 0.00710799265652895\n",
      "epoch 211 loss 0.7342082858085632 LR 0.727292537689209 LKL 0.006915748585015535\n",
      "epoch 212 loss 0.6601271033287048 LR 0.6530119180679321 LKL 0.007115173153579235\n",
      "epoch 213 loss 0.664620041847229 LR 0.6571335792541504 LKL 0.007486487273126841\n",
      "epoch 214 loss 0.7210682034492493 LR 0.7134488821029663 LKL 0.007619329262524843\n",
      "epoch 215 loss 0.7924870252609253 LR 0.7850092053413391 LKL 0.0074778227135539055\n",
      "epoch 216 loss 0.7036630511283875 LR 0.6964587569236755 LKL 0.007204318884760141\n",
      "epoch 217 loss 0.7244853377342224 LR 0.7173818349838257 LKL 0.007103514391928911\n",
      "epoch 218 loss 0.6681212186813354 LR 0.6610004901885986 LKL 0.007120730355381966\n",
      "epoch 219 loss 0.7099412083625793 LR 0.7028292417526245 LKL 0.007111950311809778\n",
      "epoch 220 loss 0.7950589656829834 LR 0.7879089713096619 LKL 0.007149992045015097\n",
      "epoch 221 loss 0.6860144138336182 LR 0.6788428425788879 LKL 0.007171569392085075\n",
      "epoch 222 loss 0.777197539806366 LR 0.7702921032905579 LKL 0.006905414164066315\n",
      "epoch 223 loss 0.6910355091094971 LR 0.6840350031852722 LKL 0.007000486366450787\n",
      "epoch 224 loss 0.6887566447257996 LR 0.6817106008529663 LKL 0.007046056911349297\n",
      "epoch 225 loss 0.7412947416305542 LR 0.7341698408126831 LKL 0.007124898489564657\n",
      "epoch 226 loss 0.7461010217666626 LR 0.7388937473297119 LKL 0.007207255810499191\n",
      "epoch 227 loss 0.6663857102394104 LR 0.6590278148651123 LKL 0.007357885595411062\n",
      "epoch 228 loss 0.7031912803649902 LR 0.6957974433898926 LKL 0.0073938206769526005\n",
      "epoch 229 loss 0.6799602508544922 LR 0.6722499132156372 LKL 0.0077103241346776485\n",
      "epoch 230 loss 0.6492671370506287 LR 0.6414158940315247 LKL 0.00785124022513628\n",
      "epoch 231 loss 0.7072005867958069 LR 0.6995320916175842 LKL 0.007668504025787115\n",
      "epoch 232 loss 0.7095102071762085 LR 0.7016122937202454 LKL 0.007897928357124329\n",
      "epoch 233 loss 0.6618443131446838 LR 0.6540082693099976 LKL 0.007836051285266876\n",
      "epoch 234 loss 0.6730656623840332 LR 0.6651135683059692 LKL 0.00795208290219307\n",
      "epoch 235 loss 0.6625950932502747 LR 0.6547033190727234 LKL 0.007891771383583546\n",
      "epoch 236 loss 0.6436461210250854 LR 0.6360179781913757 LKL 0.00762815959751606\n",
      "epoch 237 loss 0.6085495948791504 LR 0.6010054349899292 LKL 0.007544156163930893\n",
      "epoch 238 loss 0.7014898061752319 LR 0.6939065456390381 LKL 0.00758323585614562\n",
      "epoch 239 loss 0.6518067717552185 LR 0.6442380547523499 LKL 0.007568740751594305\n",
      "epoch 240 loss 0.6464025378227234 LR 0.6390655040740967 LKL 0.0073370179161429405\n",
      "epoch 241 loss 0.6776919364929199 LR 0.6704152226448059 LKL 0.0072766863740980625\n",
      "epoch 242 loss 0.6889363527297974 LR 0.6814115047454834 LKL 0.007524840533733368\n",
      "epoch 243 loss 0.5844133496284485 LR 0.5770007371902466 LKL 0.0074125975370407104\n",
      "epoch 244 loss 0.6309505701065063 LR 0.6235783100128174 LKL 0.007372281514108181\n",
      "epoch 245 loss 0.6593155860900879 LR 0.6519477367401123 LKL 0.007367833983153105\n",
      "epoch 246 loss 0.695740818977356 LR 0.6883362531661987 LKL 0.007404588162899017\n",
      "epoch 247 loss 0.6783130168914795 LR 0.6709028482437134 LKL 0.007410150021314621\n",
      "epoch 248 loss 0.621390163898468 LR 0.6138734221458435 LKL 0.007516717538237572\n",
      "epoch 249 loss 0.6539248824119568 LR 0.6464161276817322 LKL 0.00750873563811183\n",
      "epoch 250 loss 0.6537986397743225 LR 0.6463484764099121 LKL 0.007450141943991184\n",
      "epoch 251 loss 0.6915595531463623 LR 0.684019148349762 LKL 0.0075404029339551926\n",
      "epoch 252 loss 0.6615619659423828 LR 0.6540135145187378 LKL 0.007548435125499964\n",
      "epoch 253 loss 0.6341006755828857 LR 0.6265413165092468 LKL 0.007559363264590502\n",
      "epoch 254 loss 0.6383844614028931 LR 0.6305760741233826 LKL 0.007808397989720106\n",
      "epoch 255 loss 0.6662658452987671 LR 0.6583970189094543 LKL 0.007868850603699684\n",
      "epoch 256 loss 0.6240038871765137 LR 0.616158664226532 LKL 0.007845250889658928\n",
      "epoch 257 loss 0.6347346901893616 LR 0.6269437670707703 LKL 0.007790904492139816\n",
      "epoch 258 loss 0.682447075843811 LR 0.6745011210441589 LKL 0.007945956662297249\n",
      "epoch 259 loss 0.6797752976417542 LR 0.6721333265304565 LKL 0.007641990669071674\n",
      "epoch 260 loss 0.6740115880966187 LR 0.6661556959152222 LKL 0.007855880074203014\n",
      "epoch 261 loss 0.6142539978027344 LR 0.6064535975456238 LKL 0.00780041329562664\n",
      "epoch 262 loss 0.6374221444129944 LR 0.6292765140533447 LKL 0.008145627565681934\n",
      "epoch 263 loss 0.6240038275718689 LR 0.6161457300186157 LKL 0.00785809475928545\n",
      "epoch 264 loss 0.6218224763870239 LR 0.6140892505645752 LKL 0.007733226288110018\n",
      "epoch 265 loss 0.709686279296875 LR 0.7018613815307617 LKL 0.007824876345694065\n",
      "epoch 266 loss 0.6188273429870605 LR 0.6111288070678711 LKL 0.007698522415012121\n",
      "epoch 267 loss 0.7071906924247742 LR 0.6995300650596619 LKL 0.007660610135644674\n",
      "epoch 268 loss 0.5093146562576294 LR 0.5013737678527832 LKL 0.007940869778394699\n",
      "epoch 269 loss 0.6185026168823242 LR 0.6107112765312195 LKL 0.007791318465024233\n",
      "epoch 270 loss 0.5327798128128052 LR 0.5249221324920654 LKL 0.00785768125206232\n",
      "epoch 271 loss 0.7026414275169373 LR 0.694801926612854 LKL 0.007839473895728588\n",
      "epoch 272 loss 0.6199382543563843 LR 0.6121394634246826 LKL 0.007798801641911268\n",
      "epoch 273 loss 0.6008803844451904 LR 0.5929509997367859 LKL 0.00792936235666275\n",
      "epoch 274 loss 0.5725600123405457 LR 0.5643750429153442 LKL 0.00818499643355608\n",
      "epoch 275 loss 0.6339905261993408 LR 0.6260940432548523 LKL 0.00789649412035942\n",
      "epoch 276 loss 0.5758086442947388 LR 0.5680020451545715 LKL 0.007806603331118822\n",
      "epoch 277 loss 0.6353322863578796 LR 0.6276796460151672 LKL 0.00765263894572854\n",
      "epoch 278 loss 0.6496245861053467 LR 0.6419786810874939 LKL 0.007645886391401291\n",
      "epoch 279 loss 0.6587561368942261 LR 0.6509846448898315 LKL 0.0077714985236525536\n",
      "epoch 280 loss 0.6300529837608337 LR 0.6224378943443298 LKL 0.007615118753165007\n",
      "epoch 281 loss 0.5796152353286743 LR 0.5715976357460022 LKL 0.00801759958267212\n",
      "epoch 282 loss 0.5753372311592102 LR 0.567234218120575 LKL 0.008102989755570889\n",
      "epoch 283 loss 0.6243504881858826 LR 0.6162598729133606 LKL 0.00809062272310257\n",
      "epoch 284 loss 0.6014963388442993 LR 0.5934047102928162 LKL 0.00809160154312849\n",
      "epoch 285 loss 0.5832288861274719 LR 0.5750364065170288 LKL 0.008192481473088264\n",
      "epoch 286 loss 0.6058177351951599 LR 0.5975779294967651 LKL 0.00823978427797556\n",
      "epoch 287 loss 0.6688159108161926 LR 0.6604765057563782 LKL 0.008339399471879005\n",
      "epoch 288 loss 0.5864289402961731 LR 0.5782387256622314 LKL 0.008190191350877285\n",
      "epoch 289 loss 0.6090238690376282 LR 0.600919246673584 LKL 0.008104615844786167\n",
      "epoch 290 loss 0.6135454773902893 LR 0.6051239967346191 LKL 0.008421463891863823\n",
      "epoch 291 loss 0.601176381111145 LR 0.5926597118377686 LKL 0.008516646921634674\n",
      "epoch 292 loss 0.6015349626541138 LR 0.5929129719734192 LKL 0.008622003719210625\n",
      "epoch 293 loss 0.6140443682670593 LR 0.6052589416503906 LKL 0.008785399608314037\n",
      "epoch 294 loss 0.647987961769104 LR 0.6393997669219971 LKL 0.008588198572397232\n",
      "epoch 295 loss 0.6210188865661621 LR 0.6123406887054443 LKL 0.008678224869072437\n",
      "epoch 296 loss 0.6279358863830566 LR 0.6193511486053467 LKL 0.008584734052419662\n",
      "epoch 297 loss 0.5894437432289124 LR 0.5811256170272827 LKL 0.00831813458353281\n",
      "epoch 298 loss 0.6069826483726501 LR 0.5989110469818115 LKL 0.008071613498032093\n",
      "epoch 299 loss 0.5891447067260742 LR 0.5809818506240845 LKL 0.00816284865140915\n",
      "epoch 300 loss 0.6204906702041626 LR 0.6122996211051941 LKL 0.008191071450710297\n",
      "0\n",
      "epoch 301 loss 0.5702410340309143 LR 0.5621288418769836 LKL 0.008112186565995216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-9-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-9-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-15-63ddef2dc04f>:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-15-63ddef2dc04f>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 302 loss 0.5570665001869202 LR 0.5488062500953674 LKL 0.008260239847004414\n",
      "epoch 303 loss 0.5858306884765625 LR 0.5777614116668701 LKL 0.008069301024079323\n",
      "epoch 304 loss 0.5674880146980286 LR 0.5594044327735901 LKL 0.008083558641374111\n",
      "epoch 305 loss 0.5574496388435364 LR 0.5493878126144409 LKL 0.00806181039661169\n",
      "epoch 306 loss 0.6240548491477966 LR 0.6157549619674683 LKL 0.008299908600747585\n",
      "epoch 307 loss 0.573070228099823 LR 0.5648547410964966 LKL 0.00821551214903593\n",
      "epoch 308 loss 0.6161563992500305 LR 0.6077340245246887 LKL 0.008422376587986946\n",
      "epoch 309 loss 0.5505664944648743 LR 0.5419245362281799 LKL 0.008641960099339485\n",
      "epoch 310 loss 0.5987535715103149 LR 0.5902228951454163 LKL 0.008530697785317898\n",
      "epoch 311 loss 0.5731447339057922 LR 0.5644463300704956 LKL 0.008698424324393272\n",
      "epoch 312 loss 0.5786193609237671 LR 0.5699713826179504 LKL 0.008647997863590717\n",
      "epoch 313 loss 0.6337071657180786 LR 0.6250166893005371 LKL 0.008690498769283295\n",
      "epoch 314 loss 0.6000240445137024 LR 0.5911426544189453 LKL 0.008881377056241035\n",
      "epoch 315 loss 0.6123438477516174 LR 0.6035656929016113 LKL 0.008778140880167484\n",
      "epoch 316 loss 0.6148239970207214 LR 0.6057549715042114 LKL 0.009068998508155346\n",
      "epoch 317 loss 0.5769190788269043 LR 0.5679507255554199 LKL 0.008968370966613293\n",
      "epoch 318 loss 0.6218156814575195 LR 0.6127682328224182 LKL 0.009047475643455982\n",
      "epoch 319 loss 0.5374679565429688 LR 0.5282491445541382 LKL 0.009218793362379074\n",
      "epoch 320 loss 0.5736557245254517 LR 0.5644620656967163 LKL 0.00919363833963871\n",
      "epoch 321 loss 0.5902270674705505 LR 0.5810955762863159 LKL 0.009131500497460365\n",
      "epoch 322 loss 0.6076517701148987 LR 0.5986292958259583 LKL 0.009022503159940243\n",
      "epoch 323 loss 0.5624455809593201 LR 0.5531712770462036 LKL 0.009274328127503395\n",
      "epoch 324 loss 0.6121517419815063 LR 0.602886974811554 LKL 0.009264758788049221\n",
      "epoch 325 loss 0.5247781276702881 LR 0.5156074166297913 LKL 0.0091707156971097\n",
      "epoch 326 loss 0.5650567412376404 LR 0.5557681322097778 LKL 0.009288592264056206\n",
      "epoch 327 loss 0.5519181489944458 LR 0.5427318215370178 LKL 0.00918632373213768\n",
      "epoch 328 loss 0.624502956867218 LR 0.6156058311462402 LKL 0.00889710895717144\n",
      "epoch 329 loss 0.6403650045394897 LR 0.6311642527580261 LKL 0.009200734086334705\n",
      "epoch 330 loss 0.5285265445709229 LR 0.5195169448852539 LKL 0.009009580127894878\n",
      "epoch 331 loss 0.5413355827331543 LR 0.5324910879135132 LKL 0.008844495750963688\n",
      "epoch 332 loss 0.5452037453651428 LR 0.5362887382507324 LKL 0.00891503132879734\n",
      "epoch 333 loss 0.5492424368858337 LR 0.5401554107666016 LKL 0.009087025187909603\n",
      "epoch 334 loss 0.5932233333587646 LR 0.5842335224151611 LKL 0.008989783003926277\n",
      "epoch 335 loss 0.5474376678466797 LR 0.5384253859519958 LKL 0.009012293070554733\n",
      "epoch 336 loss 0.5546934604644775 LR 0.5456950664520264 LKL 0.008998376317322254\n",
      "epoch 337 loss 0.6194626092910767 LR 0.6102622151374817 LKL 0.009200389496982098\n",
      "epoch 338 loss 0.5606013536453247 LR 0.5515577793121338 LKL 0.009043569676578045\n",
      "epoch 339 loss 0.6298958659172058 LR 0.6208484172821045 LKL 0.009047474712133408\n",
      "epoch 340 loss 0.5731679201126099 LR 0.5640072822570801 LKL 0.009160630404949188\n",
      "epoch 341 loss 0.5753223896026611 LR 0.5659742951393127 LKL 0.009348082356154919\n",
      "epoch 342 loss 0.513444185256958 LR 0.5041759014129639 LKL 0.009268289431929588\n",
      "epoch 343 loss 0.5512623190879822 LR 0.5421253442764282 LKL 0.009136971086263657\n",
      "epoch 344 loss 0.5745493769645691 LR 0.5650796890258789 LKL 0.00946970097720623\n",
      "epoch 345 loss 0.5856173038482666 LR 0.5763401985168457 LKL 0.009277120232582092\n",
      "epoch 346 loss 0.5562412142753601 LR 0.5471235513687134 LKL 0.009117642417550087\n",
      "epoch 347 loss 0.5523308515548706 LR 0.5430680513381958 LKL 0.009262777864933014\n",
      "epoch 348 loss 0.5408087968826294 LR 0.5313551425933838 LKL 0.009453658014535904\n",
      "epoch 349 loss 0.5334833264350891 LR 0.5240544676780701 LKL 0.00942885223776102\n",
      "epoch 350 loss 0.5478435158729553 LR 0.5383594632148743 LKL 0.009484048001468182\n",
      "epoch 351 loss 0.5361818671226501 LR 0.5266832113265991 LKL 0.009498677216470242\n",
      "epoch 352 loss 0.5508943200111389 LR 0.5414789915084839 LKL 0.00941532477736473\n",
      "epoch 353 loss 0.5926212072372437 LR 0.5833783149719238 LKL 0.00924287736415863\n",
      "epoch 354 loss 0.49494561553001404 LR 0.48554080724716187 LKL 0.009404820390045643\n",
      "epoch 355 loss 0.5496072173118591 LR 0.5401320457458496 LKL 0.00947517529129982\n",
      "epoch 356 loss 0.5691140294075012 LR 0.5595625638961792 LKL 0.00955146923661232\n",
      "epoch 357 loss 0.5734996199607849 LR 0.5640978813171387 LKL 0.009401742368936539\n",
      "epoch 358 loss 0.5872185826301575 LR 0.5775700807571411 LKL 0.009648493491113186\n",
      "epoch 359 loss 0.5593659281730652 LR 0.5498110055923462 LKL 0.009554946795105934\n",
      "epoch 360 loss 0.532260537147522 LR 0.522876501083374 LKL 0.009384025819599628\n",
      "epoch 361 loss 0.5592733025550842 LR 0.5498386025428772 LKL 0.009434682317078114\n",
      "epoch 362 loss 0.4872572720050812 LR 0.47800755500793457 LKL 0.009249724447727203\n",
      "epoch 363 loss 0.5207453370094299 LR 0.5114092826843262 LKL 0.009336058050394058\n",
      "epoch 364 loss 0.5545011758804321 LR 0.5453348755836487 LKL 0.009166271425783634\n",
      "epoch 365 loss 0.5043199062347412 LR 0.49499136209487915 LKL 0.009328525513410568\n",
      "epoch 366 loss 0.5153605937957764 LR 0.5061238408088684 LKL 0.009236747398972511\n",
      "epoch 367 loss 0.5170648097991943 LR 0.5078632831573486 LKL 0.009201504290103912\n",
      "epoch 368 loss 0.5272819399833679 LR 0.5180377960205078 LKL 0.009244141168892384\n",
      "epoch 369 loss 0.5633256435394287 LR 0.554162323474884 LKL 0.009163299575448036\n",
      "epoch 370 loss 0.506510853767395 LR 0.4973633289337158 LKL 0.0091475248336792\n",
      "epoch 371 loss 0.5771529078483582 LR 0.5677662491798401 LKL 0.009386676363646984\n",
      "epoch 372 loss 0.5589463114738464 LR 0.5496982932090759 LKL 0.009247991256415844\n",
      "epoch 373 loss 0.591256320476532 LR 0.5820509195327759 LKL 0.009205423295497894\n",
      "epoch 374 loss 0.5145027041435242 LR 0.5052168965339661 LKL 0.009285805746912956\n",
      "epoch 375 loss 0.5891492962837219 LR 0.5798372030258179 LKL 0.009312080219388008\n",
      "epoch 376 loss 0.6129715442657471 LR 0.6036484241485596 LKL 0.009323092177510262\n",
      "epoch 377 loss 0.5321651697158813 LR 0.5229193568229675 LKL 0.009245787747204304\n",
      "epoch 378 loss 0.5301657915115356 LR 0.5206837058067322 LKL 0.009482083842158318\n",
      "epoch 379 loss 0.484451562166214 LR 0.47505420446395874 LKL 0.009397370740771294\n",
      "epoch 380 loss 0.5104411840438843 LR 0.5008623003959656 LKL 0.009578902274370193\n",
      "epoch 381 loss 0.4938249886035919 LR 0.48423904180526733 LKL 0.009585960768163204\n",
      "epoch 382 loss 0.49329566955566406 LR 0.48353883624076843 LKL 0.009756820276379585\n",
      "epoch 383 loss 0.6121454238891602 LR 0.6026754975318909 LKL 0.009469928219914436\n",
      "epoch 384 loss 0.5923247337341309 LR 0.582846999168396 LKL 0.009477755054831505\n",
      "epoch 385 loss 0.590252161026001 LR 0.5808711051940918 LKL 0.009381069801747799\n",
      "epoch 386 loss 0.5090864300727844 LR 0.49966222047805786 LKL 0.009424224495887756\n",
      "epoch 387 loss 0.5925401449203491 LR 0.583109438419342 LKL 0.00943073257803917\n",
      "epoch 388 loss 0.5799795985221863 LR 0.5704664587974548 LKL 0.009513136930763721\n",
      "epoch 389 loss 0.4792082905769348 LR 0.4695146083831787 LKL 0.009693676605820656\n",
      "epoch 390 loss 0.49557432532310486 LR 0.4858495891094208 LKL 0.009724738076329231\n",
      "epoch 391 loss 0.5423930287361145 LR 0.5329728126525879 LKL 0.009420203976333141\n",
      "epoch 392 loss 0.5447872877120972 LR 0.535306453704834 LKL 0.009480859152972698\n",
      "epoch 393 loss 0.5364599227905273 LR 0.5269593000411987 LKL 0.009500603191554546\n",
      "epoch 394 loss 0.5257871747016907 LR 0.5163785219192505 LKL 0.009408639743924141\n",
      "epoch 395 loss 0.5585857629776001 LR 0.5491106510162354 LKL 0.00947509240359068\n",
      "epoch 396 loss 0.46791672706604004 LR 0.45839813351631165 LKL 0.00951860286295414\n",
      "epoch 397 loss 0.5028203129768372 LR 0.49324727058410645 LKL 0.00957306008785963\n",
      "epoch 398 loss 0.5172802209854126 LR 0.5074679851531982 LKL 0.009812247939407825\n",
      "epoch 399 loss 0.5338867902755737 LR 0.5240257978439331 LKL 0.009860982187092304\n",
      "epoch 400 loss 0.5566375255584717 LR 0.5468379259109497 LKL 0.009799601510167122\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin Baek\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-9-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-9-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-15-63ddef2dc04f>:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-15-63ddef2dc04f>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 401 loss 0.4910261034965515 LR 0.48141375184059143 LKL 0.00961235724389553\n",
      "epoch 402 loss 0.6134294867515564 LR 0.603863000869751 LKL 0.00956646353006363\n",
      "epoch 403 loss 0.5109378099441528 LR 0.501406192779541 LKL 0.00953160785138607\n",
      "epoch 404 loss 0.5978473424911499 LR 0.5882593393325806 LKL 0.009587973356246948\n",
      "epoch 405 loss 0.445570170879364 LR 0.4360906481742859 LKL 0.00947953574359417\n",
      "epoch 406 loss 0.5709347724914551 LR 0.5613701939582825 LKL 0.009564594365656376\n",
      "epoch 407 loss 0.5703397393226624 LR 0.5607577562332153 LKL 0.009581982158124447\n",
      "epoch 408 loss 0.5142349600791931 LR 0.504578709602356 LKL 0.009656227193772793\n",
      "epoch 409 loss 0.4452798664569855 LR 0.43544137477874756 LKL 0.009838498197495937\n",
      "epoch 410 loss 0.47873347997665405 LR 0.4686812162399292 LKL 0.010052253492176533\n",
      "epoch 411 loss 0.4959622621536255 LR 0.48599353432655334 LKL 0.009968739934265614\n",
      "epoch 412 loss 0.4812096059322357 LR 0.47117480635643005 LKL 0.010034812614321709\n",
      "epoch 413 loss 0.4351561367511749 LR 0.42532891035079956 LKL 0.009827221743762493\n",
      "epoch 414 loss 0.5377464294433594 LR 0.5277819633483887 LKL 0.009964480064809322\n",
      "epoch 415 loss 0.5350514650344849 LR 0.525047779083252 LKL 0.010003709234297276\n",
      "epoch 416 loss 0.45460426807403564 LR 0.44468721747398376 LKL 0.009917058050632477\n",
      "epoch 417 loss 0.5168620944023132 LR 0.50691819190979 LKL 0.009943900629878044\n",
      "epoch 418 loss 0.5725182294845581 LR 0.5625783205032349 LKL 0.009939901530742645\n",
      "epoch 419 loss 0.551403284072876 LR 0.5415167808532715 LKL 0.009886516258120537\n",
      "epoch 420 loss 0.5416487455368042 LR 0.5314905047416687 LKL 0.010158214718103409\n",
      "epoch 421 loss 0.46300315856933594 LR 0.45288926362991333 LKL 0.010113888420164585\n",
      "epoch 422 loss 0.46355459094047546 LR 0.4536284804344177 LKL 0.009926099330186844\n",
      "epoch 423 loss 0.5332314372062683 LR 0.5232720375061035 LKL 0.009959411807358265\n",
      "epoch 424 loss 0.5021682381629944 LR 0.49210041761398315 LKL 0.01006783451884985\n",
      "epoch 425 loss 0.5488616228103638 LR 0.5388582944869995 LKL 0.010003341361880302\n",
      "epoch 426 loss 0.5807584524154663 LR 0.5706919431686401 LKL 0.010066510178148746\n",
      "epoch 427 loss 0.4505055248737335 LR 0.4404189884662628 LKL 0.01008654199540615\n",
      "epoch 428 loss 0.5193739533424377 LR 0.5091921091079712 LKL 0.010181847028434277\n",
      "epoch 429 loss 0.46256551146507263 LR 0.45232152938842773 LKL 0.010243983939290047\n",
      "epoch 430 loss 0.5080690383911133 LR 0.49777325987815857 LKL 0.010295807383954525\n",
      "epoch 431 loss 0.5790806412696838 LR 0.5688071250915527 LKL 0.010273494757711887\n",
      "epoch 432 loss 0.5424655675888062 LR 0.5320282578468323 LKL 0.01043730415403843\n",
      "epoch 433 loss 0.5679787993431091 LR 0.557746171951294 LKL 0.010232601314783096\n",
      "epoch 434 loss 0.4655093848705292 LR 0.45503756403923035 LKL 0.010471833869814873\n",
      "epoch 435 loss 0.5149667263031006 LR 0.5045506954193115 LKL 0.010416033677756786\n",
      "epoch 436 loss 0.5016273856163025 LR 0.4912811517715454 LKL 0.0103462478145957\n",
      "epoch 437 loss 0.5708879232406616 LR 0.5605872869491577 LKL 0.01030061673372984\n",
      "epoch 438 loss 0.541162371635437 LR 0.531020998954773 LKL 0.010141347534954548\n",
      "epoch 439 loss 0.49525022506713867 LR 0.48492810130119324 LKL 0.01032213494181633\n",
      "epoch 440 loss 0.49689868092536926 LR 0.4870144724845886 LKL 0.00988420844078064\n",
      "epoch 441 loss 0.5523315072059631 LR 0.5423109531402588 LKL 0.01002057921141386\n",
      "epoch 442 loss 0.5134376883506775 LR 0.5035747289657593 LKL 0.009862937033176422\n",
      "epoch 443 loss 0.4661886692047119 LR 0.4562734365463257 LKL 0.009915224276483059\n",
      "epoch 444 loss 0.47485584020614624 LR 0.4649796485900879 LKL 0.009876198135316372\n",
      "epoch 445 loss 0.4979507029056549 LR 0.48798927664756775 LKL 0.009961419738829136\n",
      "epoch 446 loss 0.5687420964241028 LR 0.558760404586792 LKL 0.009981676004827023\n",
      "epoch 447 loss 0.5080603361129761 LR 0.4979546070098877 LKL 0.010105730034410954\n",
      "epoch 448 loss 0.4696381092071533 LR 0.4594264030456543 LKL 0.010211716406047344\n",
      "epoch 449 loss 0.5066748857498169 LR 0.4965846538543701 LKL 0.010090217925608158\n",
      "epoch 450 loss 0.4772402048110962 LR 0.46715331077575684 LKL 0.010086890310049057\n",
      "epoch 451 loss 0.5384613275527954 LR 0.5281912684440613 LKL 0.010270077735185623\n",
      "epoch 452 loss 0.5025001764297485 LR 0.49217772483825684 LKL 0.010322462767362595\n",
      "epoch 453 loss 0.5289639830589294 LR 0.5186651349067688 LKL 0.010298858396708965\n",
      "epoch 454 loss 0.5412607192993164 LR 0.5308345556259155 LKL 0.010426183231174946\n",
      "epoch 455 loss 0.5399288535118103 LR 0.5292614698410034 LKL 0.010667395778000355\n",
      "epoch 456 loss 0.46764037013053894 LR 0.4569851756095886 LKL 0.010655195452272892\n",
      "epoch 457 loss 0.5208913683891296 LR 0.5104141235351562 LKL 0.010477269999682903\n",
      "epoch 458 loss 0.45978888869285583 LR 0.44947540760040283 LKL 0.010313475504517555\n",
      "epoch 459 loss 0.5187361836433411 LR 0.5084543824195862 LKL 0.010281775146722794\n",
      "epoch 460 loss 0.5178604125976562 LR 0.507575273513794 LKL 0.010285161435604095\n",
      "epoch 461 loss 0.4858967661857605 LR 0.475663423538208 LKL 0.010233343578875065\n",
      "epoch 462 loss 0.4989359378814697 LR 0.488555908203125 LKL 0.010380037128925323\n",
      "epoch 463 loss 0.47357168793678284 LR 0.46321770548820496 LKL 0.010353985242545605\n",
      "epoch 464 loss 0.478517085313797 LR 0.468222975730896 LKL 0.01029410120099783\n",
      "epoch 465 loss 0.49559080600738525 LR 0.4852246642112732 LKL 0.010366149246692657\n",
      "epoch 466 loss 0.4867522716522217 LR 0.47642987966537476 LKL 0.010322397574782372\n",
      "epoch 467 loss 0.5150933861732483 LR 0.5047177076339722 LKL 0.010375656187534332\n",
      "epoch 468 loss 0.4232025742530823 LR 0.4126594364643097 LKL 0.010543126612901688\n",
      "epoch 469 loss 0.49380552768707275 LR 0.48322635889053345 LKL 0.010579177178442478\n",
      "epoch 470 loss 0.513494074344635 LR 0.5026739239692688 LKL 0.010820162482559681\n",
      "epoch 471 loss 0.5253483057022095 LR 0.5145132541656494 LKL 0.010835045948624611\n",
      "epoch 472 loss 0.4802757203578949 LR 0.46940159797668457 LKL 0.010874125175178051\n",
      "epoch 473 loss 0.515852153301239 LR 0.5049476027488708 LKL 0.010904544033110142\n",
      "epoch 474 loss 0.584343671798706 LR 0.5732706785202026 LKL 0.011073008179664612\n",
      "epoch 475 loss 0.486855685710907 LR 0.47600364685058594 LKL 0.01085203792899847\n",
      "epoch 476 loss 0.5089249014854431 LR 0.4978628158569336 LKL 0.011062059551477432\n",
      "epoch 477 loss 0.4897642135620117 LR 0.4788336157798767 LKL 0.010930598713457584\n",
      "epoch 478 loss 0.4023459553718567 LR 0.391598105430603 LKL 0.010747864842414856\n",
      "epoch 479 loss 0.4791506826877594 LR 0.4684288501739502 LKL 0.010721832513809204\n",
      "epoch 480 loss 0.4425092935562134 LR 0.4318515360355377 LKL 0.01065774355083704\n",
      "epoch 481 loss 0.5154929161071777 LR 0.5047643780708313 LKL 0.010728542692959309\n",
      "epoch 482 loss 0.49208545684814453 LR 0.4814738631248474 LKL 0.010611578822135925\n",
      "epoch 483 loss 0.506927490234375 LR 0.4960239827632904 LKL 0.010903504677116871\n",
      "epoch 484 loss 0.4692803621292114 LR 0.4584351181983948 LKL 0.010845230892300606\n",
      "epoch 485 loss 0.4460502862930298 LR 0.43517816066741943 LKL 0.010872132144868374\n",
      "epoch 486 loss 0.5661273002624512 LR 0.5550605654716492 LKL 0.011066725477576256\n",
      "epoch 487 loss 0.5054828524589539 LR 0.49431726336479187 LKL 0.011165601201355457\n",
      "epoch 488 loss 0.5183576941490173 LR 0.5073081254959106 LKL 0.01104955468326807\n",
      "epoch 489 loss 0.4490431845188141 LR 0.4381629526615143 LKL 0.01088023278862238\n",
      "epoch 490 loss 0.4856582581996918 LR 0.47475767135620117 LKL 0.010900578461587429\n",
      "epoch 491 loss 0.43869832158088684 LR 0.4278448224067688 LKL 0.010853510349988937\n",
      "epoch 492 loss 0.44909384846687317 LR 0.43820920586586 LKL 0.01088463980704546\n",
      "epoch 493 loss 0.44314947724342346 LR 0.4322485327720642 LKL 0.010900931432843208\n",
      "epoch 494 loss 0.42312946915626526 LR 0.4122258424758911 LKL 0.010903620161116123\n",
      "epoch 495 loss 0.433817982673645 LR 0.4230867624282837 LKL 0.010731233283877373\n",
      "epoch 496 loss 0.4400656521320343 LR 0.4292624592781067 LKL 0.01080319657921791\n",
      "epoch 497 loss 0.4256106913089752 LR 0.4146840274333954 LKL 0.010926668532192707\n",
      "epoch 498 loss 0.4722367227077484 LR 0.461270809173584 LKL 0.010965925641357899\n",
      "epoch 499 loss 0.5047122240066528 LR 0.4938333034515381 LKL 0.010878944769501686\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    model = Model()\n",
    "    for epoch in range(500):\n",
    "        model.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "#     # inputs must be floats\n",
    "#     if greedy:\n",
    "#         return mu_x,mu_y\n",
    "#     mean = [mu_x, mu_y]\n",
    "#     sigma_x = sigma_x * np.sqrt(hp.temperature)\n",
    "#     sigma_y = sigma_y * np.sqrt(hp.temperature)\n",
    "#     cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],\\\n",
    "#         [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "#     import pdb; pdb.set_trace\n",
    "#     x = np.random.multivariate_normal(mean, cov, 1)\n",
    "#     x = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "#     return x[0][0], x[0][1]\n",
    "\n",
    "def make_image(sequence, epoch, name='_output_'):\n",
    "    \"\"\"plot drawing with separated strokes\"\"\"\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for s in strokes:\n",
    "        plt.plot(s[:,0],-s[:,1])\n",
    "    canvas = plt.get_current_fig_manager().canvas\n",
    "    canvas.draw()\n",
    "    pil_image = PIL.Image.frombytes('RGB', canvas.get_width_height(),\n",
    "                 canvas.tostring_rgb())\n",
    "    name = str(epoch)+name+'.jpg'\n",
    "    pil_image.save(name,\"JPEG\")\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-49127ff506c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpi\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pi' is not defined"
     ]
    }
   ],
   "source": [
    "pi = pi[0, 0, :]\n",
    "print(pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
